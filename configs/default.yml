model_params:
  n_layers: 12
  n_heads: 8
  c: 256
  h: 32
  t: 256
  dropout: 0.0
  feedforward_factor: 4
  vocab_size: 8192

training_args:
  learning_rate: 0.0001
  eval_interval: 300
  max_steps: 60000
  batch_size: 64
  l2_penalty: 0.0

data_args:
  data_path: train.txt
  train_split: 0.8
